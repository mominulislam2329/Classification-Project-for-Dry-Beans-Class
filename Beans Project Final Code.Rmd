---
title: "STAT 602 Midterm Project"
author: "Md Mominul Islam (101009250)"
date: "04/03/2022"
output:
  word_document: default
  pdf_document: default
  html_document: default
---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F)
```

## Project Obejective

Koklu et al. represented multiclass classification of dry beans using computer vision and machine learning techniques and provided a method for obtaining uniform seed varieties from crop production. [Link: ttps://doi.org/10.1016/j.compag.2020.105507]. Kara et al. published the results of Principal Component Analysis (PCA) for beans and they decided that the most important variables of the first component were mass, geometric mean diameter, surface area, volume, projected area, equivalent diameter, perimeter and length. 

Our aim of this project to explore the potential of the various learning methods and give a recommendation on which type of algorithm we should further develop for the task of sorting white beans. We are expected to develop an automated method that predicts the value of a harvest from a 'population cultivation' from a single farm that has been presented at market. Since each of the beans has a different value at market the cost of a error depends on the actual type of white bean and what it is predicted class. Additionally, we will provide some measure of our predictive systems.


## Background

In a recent paper, Koklu et al. explored the possibility of using morphometric measurements on seven commonly cultivated white beans to develop an automated (or at least machine assisted) method for separating the white beans when presented at market. 

> Using certified dry bean seeds in Turkey is around 10% (Bolat et al., 2017). Dry bean cultivation in Turkey and Asian countries usually in the form of populations containing mixed species of seeds. Also, there is not much certified seed planting area (Varankaya and Ceyhan, 2012). Since different populations which contain different genotypes are cultivated, the final products contain different species of seeds. Thus, when the dry bean seeds obtained from population cultivation are released to the market without being separated by species, the market value decreases immensely (Varankaya and Ceyhan, 2012). -Murat Koklu, Ilker Ali Ozkan, Multiclass classification of dry beans using computer vision and machine learning techniques, Computers and Electronics in Agriculture, Volume 174, 2020,


Our aim of this project to explore the potential of the various learning methods and give a recommendation on which type of algorithm we should further develop for the task of sorting white beans. You are expected to develop an automated method that predicts the value of a harvest from a 'population cultivation' from a single farm that has been presented at market. 

Since each of the beans has a different value at market the cost of a error depends on the actual type of white bean and what it is predicted class.

You will be expected to provide some measure of your predictive systems.
  
## Data Resources and Structure

For this project you will be given a sample of beans that are classified into 6 varieties of white beans at a hypothetical (ie. made up by me!) local market price per lb in USD.

1. Bombay   ($5.56/lb)
2. Cali     ($6.02/lb)
3. Dermason ($1.98/lb)
4. Horoz    ($2.43/lb)
5. Seker    ($2.72/lb)
6. Sira     ($5.40/lb)

Seed weight in grams (average gram per seed)

- Seker       0.49 grams/seed
- Bombay      1.92 grams/seed
- Cali        0.61 grams/seed
- Horoz       0.52 grams/seed 
- Sira        0.38 grams/seed
- Dermason    0.28 grams/seed

(Note that there are ~453.592 grams per pound)

In the paper by Koklu and Ozkan there is a seventh variety (Barbunya) that we are ignoring for this task. 

The column Labels of the training data are as follows. Please see the provided papers for additional details.


1. Area: Area ($A$): The area of a bean zone and the number of pixels within its boundaries.
$$A=\sum_{r,c \in R} 1,$$
where $r$, $c$ is the size of the bean region.

2. Perimeter ($P$): Bean circumference is defined as the length of its border.	

3. MajorAxisLength (Major axis length ($L$)): The distance between the ends of the longest line that can be drawn from a bean.	

4. MinorAxisLength (Minor axis length ($l$)): The longest line that can be drawn from the bean while standing perpendicular to the main axis.	

6. Eccentricity ($Ec$): Eccentricity of the ellipse having the same moments as the region.	

7. ConvexArea	(Convex Area ($C$)): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.

9. Extent ($Ex$): The ratio of the pixels in the bounding box to the bean area.
$$ Ex=\frac{A}{A_b},$$

where $A_b$ is the Area of a bounding rectangle. 

10. Class: One of the six bean types/varieties (BOMBAY, CALI, DERMASON, HOROZ, SEKER, SIRA)

```{r, warning=FALSE, message=FALSE}
#Loading Libraries
library(knitr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(class)
library(GGally)
library(tidyverse)
library(kableExtra)
library(gridExtra)
library(reshape)
library(readr)
library(readxl)
library(patchwork)
library(ggeasy)
theme_set(theme_classic())
library(ggpubr)
library(rstatix)
```




```{r}
#Loading The data set

#######For Windows###########
original.data<- read.csv("\\Users\\md.mominul.islam\\OneDrive - South Dakota State University - SDSU\\STAT 602\\Mid Term Project\\labeled.csv")

play.data <- original.data
head(original.data,n= 3) 
sum(is.na(original.data))
bean.dat<- original.data[,-1] # Deleting the column 'X'
```

## Summary Statistics


```{r}
##Summary Statistics of our data set
summary.stats <- round(as.data.frame((bean.dat[,-8])%>% #we ignore the categorical feature 'class'
                                       psych::describe())%>%
                         dplyr::select(n,mean, sd, median, min, max,range), 2)
#write.csv (summary.stats,'summary_stat_bn.csv') wrtite the data into a table 

kable(summary.stats, 
      caption="Statistical Distribution of Features of Dry Beans",
      col.names = c("Count","Mean","Standard Deviation","Median","Minimum","Maximum","Range"))

########Summary Statistics based on Dry beans class###################
cl.summary<- bean.dat %>%
  group_by(Class) %>%
  summarise(across(where(is.numeric), mean))
kable(cl.summary)
#write.csv(cl.summary, "summary.csv")
```

The variables, Area and Convex Area, had the largest range for the bean dataset. There are large differences in the range of variables, the variables with larger ranges can dominate over those with small ranges which may lead to biased results.

## Class Covariance

```{r}
var.tab1 <- bean.dat %>%
  group_by(Class)%>%
  summarize(Area=var(Area),Perimeter=var(Perimeter),
            Maj.Axis.=var(MajorAxisLength),Min.Axis.=var(MinorAxisLength), 
            Eccentricity=min(Eccentricity), var.ConvexArea=max(ConvexArea), 
            Extent=max(Extent))


kable(var.tab1, caption = "Variance of Variables by Class")
#write.csv(var.tab1, "classcov.csv")
```

From above table, we can have an idea about the class variances which measures variability from the average or mean. The variance of each variable by class shows evidence of non-constant variance. 

## Exploratory Data Analysis

```{r}
#########Categorical Data Analysis#####################
tab.class<- table(bean.dat$Class)
tab.class
```


```{r}
#Density plot using ggplot tool for numeric variables
dp.B<- ggplot(bean.dat,aes(x=Perimeter,fill=Class))+geom_density(col=NA,alpha=0.40)
dp.C<- ggplot(bean.dat, aes(x= MajorAxisLength, fill= Class))+ geom_density(col= NA, alpha= 0.40)
dp.D<- ggplot(bean.dat, aes(x=MinorAxisLength, fill= Class))+ geom_density(col= NA, alpha= 0.40)
dp.E<- ggplot(bean.dat, aes(x= Eccentricity, fill= Class))+ geom_density(col= NA, alpha= 0.40)
dp.F<- ggplot(bean.dat, aes(x= ConvexArea, fill= Class))+ geom_density(col= NA, alpha= 0.40)
dp.G<- ggplot(bean.dat, aes(x= Extent, fill= Class))+ geom_density(col= NA, alpha= 0.40)
gridExtra::grid.arrange(dp.B,dp.C,
                        dp.D,dp.E,dp.F,
                        dp.G, ncol=2)
```

```{r}
#Histogram using base R for numeric variables
num.bean <- bean.dat [,-8] #selecting numeric variables only
####################Plotting Histogram for all Numeric Variables############
plotHist <- function(columns,bin,colours){
  par(mfrow = c(3,3))#Histogram plots to visualize the distribution of the numeric variables in the data set.
  for (i in columns) {
    hist(num.bean[,i], main = paste("Histogram of ", names(num.bean)[i]),
         nclass = bin, las = 1, col = colours, 
         xlab = paste(names(num.bean)[i]))
  }
}

plotHist(c(1:7), bin = 60, "brown")



```

The histograms from the labeled data (Figure) reveal that the variables exhibit multimodal behavior. This indicates that at least one of the bean classes differs greatly from the others. Further investigation revealed that the kind of BOMBAY beans is to blame for the multimodality.





```{r}
################################## Correlation matrix##################################
library(corrplot)
library(RColorBrewer)
M <-cor(num.bean)
# as number
corrplot(M, method="number")
```

Most of the variables except for Eccentricity and Extent are highly correlated with each other. 




## Barplot by Mean for Beans Class


```{r}
ggplot(bean.dat) + aes(x = Class, y = Area, fill = Class) + 
    stat_summary(geom = "bar", fun = "mean") + stat_summary(geom = "errorbar", 
                                                            fun.data = "mean_se", 
                                                            width = .3)
ggplot(bean.dat) + aes(x = Class, y = Perimeter, fill = Class) + 
    stat_summary(geom = "bar", fun = "mean") + stat_summary(geom = "errorbar", 
                                                            fun.data = "mean_se", 
                                                            width = .3)
ggplot(bean.dat) + aes(x = Class, y = MajorAxisLength, fill = Class) + 
    stat_summary(geom = "bar", fun = "mean") + stat_summary(geom = "errorbar", 
                                                            fun.data = "mean_se", 
                                                            width = .3)
ggplot(bean.dat) + aes(x = Class, y = MinorAxisLength, fill = Class) + 
    stat_summary(geom = "bar", fun = "mean") + stat_summary(geom = "errorbar", 
                                                            fun.data = "mean_se", 
                                                            width = .3)
ggplot(bean.dat) + aes(x = Class, y = Eccentricity, fill = Class) + 
    stat_summary(geom = "bar", fun = "mean") + stat_summary(geom = "errorbar", 
                                                            fun.data = "mean_se", 
                                                            width = .3)
ggplot(bean.dat) + aes(x = Class, y = ConvexArea, fill = Class) + 
    stat_summary(geom = "bar", fun = "mean") + stat_summary(geom = "errorbar", 
                                                            fun.data = "mean_se", 
                                                            width = .3)
ggplot(bean.dat) + aes(x = Class, y = Extent, fill = Class) + 
    stat_summary(geom = "bar", fun = "mean") + stat_summary(geom = "errorbar", 
                                                            fun.data = "mean_se", 
                                                            width = .3)

  
```

## Boxplot by Beans Class


```{r}
# Box plot for Zone Area Based on Class of beans
ggplot(bean.dat, aes(x=as.factor(Class), y=Area, fill = as.factor(Class))) + geom_boxplot()+
  labs(title="Bean Zone Area",x="Beans Class", y = "Beans Area")
# Box plot for perimeter Based on Class of beans
ggplot(bean.dat, aes(x=as.factor(Class), y=Perimeter, fill = as.factor(Class))) + geom_boxplot()+ labs(title="Bean Circumference",x="Beans Class", y = "Beans Area")

# Box plot for MajorAxisLength Based on Class of beans
ggplot(bean.dat, aes(x=as.factor(Class), y=MajorAxisLength, fill = as.factor(Class))) + geom_boxplot()+ labs(title="Major Axis Length of Beans",x="Beans Class", y = "Major Axis Length")

# Box plot for MinorAxisLength Based on Class of beans
ggplot(bean.dat, aes(x=as.factor(Class), y=MinorAxisLength, fill = as.factor(Class))) + geom_boxplot()+ labs(title="Minor Axis Length of Beans",x="Beans Class", y = "Minor Axis Length")

# Box plot for Eccentricity Based on Class of beans
ggplot(bean.dat, aes(x=as.factor(Class), y=Eccentricity, fill = as.factor(Class))) + geom_boxplot()+ labs(title="Eccentricity of Beans",x="Beans Class", y = "Eccentricity")

# Box plot for ConvexArea Based on Class of beans
ggplot(bean.dat, aes(x=as.factor(Class), y=ConvexArea, fill = as.factor(Class))) + geom_boxplot()+ labs(title="Convex Area of Beans",x="Beans Class", y = "Convex Area")

# Box plot for Extent Based on Class of beans
ggplot(bean.dat, aes(x=as.factor(Class), y=Extent, fill = as.factor(Class))) + geom_boxplot()+ labs(title="Extent of Beans",x="Beans Class", y = "Extent Parameter")

```


The three different boxplots show us that the length of each plot clearly differs. This is an indication of non-equal variances.



## Pairs Plot

```{r}
ma <- as.matrix(play.data[, 2:8]) # convert to matrix
pairs(ma,
      col = 'blue',
      main = 'Pairs plot of Numeric Variables of Beans Data')
```


The way to interpret the matrix is as follows:

- The variable names are shown along the diagonals boxes.

- All other boxes display a scatterplot of the relationship between each pairwise combination of variables. For example, the box in the top right corner of the matrix displays a scatterplot of values for Area and Perimeter. 

- This single plot gives us an idea of the relationship between each pair of variables in our dataset. For example, var1 and var2 seem to be positively correlated while var1 and var3 seem to have little to no correlation.


## Analysis of Variance (One-way Anova)

One-way analysis of variance (ANOVA), also known as one-factor ANOVA, is an extension of independent two-samples t-test for comparing means in a situation where there are more than two groups. In one-way ANOVA, the data is organized into several groups base on one single grouping variable (also called factor variable).We have 5 different classes of beans. 


```{r}
##Data Sorting
order.data <- bean.dat %>%  # pipe operator; data is send to the next step
  arrange(Class) # sort in ascending order; desc(Class) for descending order
#str(order.data)  # structure of data object, data types for each column
order.data$Class <- as.factor(order.data$Class)
#str(order.data) # double checking data type
head(order.data)
```

```{r}
# Compute the analysis of variance for numeric variables against different bean class
area.aov <- summary(aov(Area~Class, data = order.data))
Perimeter.aov <- summary(aov(Perimeter ~ Class, data = order.data))
maj.aov <- summary(aov(MajorAxisLength ~ Class, data = order.data))
minor.aov <- summary(aov( MinorAxisLength ~ Class, data = order.data))
ecc.aov <- summary(aov(Eccentricity ~ Class, data = order.data))
cv.aov <- summary(aov(ConvexArea ~ Class, data = order.data))
ext.aov <- summary(aov(Extent ~ Class, data = order.data))
aov.df <- c(area.aov,Perimeter.aov,
            maj.aov,minor.aov,
            ecc.aov,cv.aov,
            ext.aov)

### Tables of anova for all numeric varibles
anova.data <- data.frame(
  Parameter = c("DF","Sum Square","Mean Square","F Statistic","p-Value"), 
  Area = c(5, 6.87e+12,1.397e+12,10877,2e-16),
  Perimeter = c(5,285666122,57133224,2221, 2e-16),
  MajorAxisLength = c(5,42590497,8518099,6523,2e-16),
  MinorAxisLength = c(5,14591140,2918228,5658,2e-16),
  Eccentrcity = c(5,22.933,4.587,1715,2e-16),
  ConvexArea = c(5,7.10e+12,1.42e+12,8442,2e-16),
  Extent = c(5,1.707,0.3414,157.1,2e-16))
  
anova.data

```

## Various Approach 



## LDA Method

Linear discriminant analysis (LDA) is a statistical technique used in pattern recognition and machine learning to determine a linear combination of characteristics that classifies or distinguishes two or more classes of objects or occurrences. The resultant combination may be used as a linear classifier or, more often, for pre-classification dimensionality reduction.


```{r,message=FALSE,warning=FALSE}
set.seed(444)
library(caret)
index <- createDataPartition(y = bean.dat$Class, p = 0.6, list = F)

train <- bean.dat[index, ]
test <- bean.dat[-index, ]

nrow(train) / nrow(bean.dat)
```


```{r,message=FALSE,warning=FALSE}
library(MASS)
##LDA Approach
linear <- lda(Class~., train)
#plot(linear, col=as.numeric(train$Class)) # assign color code based on factor code
linear$scaling #Ld1, ld2 parameters
linear$svd # proportion of trace
```

This means that the first discriminant function is a linear combination of the variables: $$6.743*10^-5∗Area + 1.089*10^-4∗Perimeter ..........+5.721*10^-1∗Extent$$. 

For  convenience, the value for  each discriminant function (eg. the first discriminant function) are scaled so that their mean value is zero and its 
variance is one.

The number of discriminant functions that can be extracted depends on the number of groups and the number of variables – it is the lesser of the degrees of freedom for groups (number of groups minus one) and the number of variables (Tabachnic'k & Fidell 1996). The scaling coefficients show, LD1 is largely explained by the variable Sepal 'Eccentricity'.

The “proportion of trace” that is printed when I type “linear” (the variable returned by the lda() function) is the percentage separation achieved by each discriminant function. For example, for the Beans Data we get the same values as just calculated (84.224811%, 33.795729%, 12.531932%,  6.267914% and 1.519649%).

Proportion of trace is the percent explained by the LDA model. We can see that LD1 Model explains 0.8410 of the model.

```{r}
#Confusion matrix and accuracy – training data
p1 <- predict(linear, train)$class
lda.pred.trn <- predict(linear, train)
tab <- table(Predicted = p1, Actual = train$Class)
tab
#write.csv(tab,'lda_train.csv')
##Accuracy of the table
sum(diag(tab))/sum(tab)

#Confusion matrix and accuracy – testing data
p2 <- predict(linear, test)$class
lda.pred.tst <- predict(linear, test)
tab1 <- table(Predicted = p2, Actual = test$Class)
tab1
#write.csv(tab1,'lda_test.csv')
sum(diag(tab1))/sum(tab1)
```

## Price Prediction Using LDA Model

## Price prediction on Train Data

```{r}
actual.class <- train$Class
tr.df.lda <- as.data.frame(lda.pred.trn$class)
tr.df.lda$actual.class <- actual.class
head(tr.df.lda)
```


```{r}
#Price Conversion for all beans
PriceBombay = (5.56/453.592)*1.92
PriceCali = (6.02/453.592)*0.61
PriceDermason = (1.98/453.592)*0.28
PriceHoroz = (2.43/453.592)*0.52
PriceSeker = (2.72/453.592)*0.49
PriceSira = (5.40/453.592)*0.38
```


## Total Price of Actual Beans Class on Train Data


```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#Actual quantity of all beans on train data
tr.actual=as.matrix(table(actual.class))

#Actual weight(in Grams) of all beans on train Data
tr.actual.weightg = (tr.actual[1]*1.92)+
  (tr.actual[2]*0.61)+
  (tr.actual[3]*0.28)+
  (tr.actual[4]*0.52)+
  (tr.actual[5]*0.49)+
  (tr.actual[6]*0.38)

#Actual weight(in lbs) of all beans on train Data
tr.actual.weightlb = tr.actual.weightg/453.592

#Total price of Beans on Train Data
tr.actual.TotPr = (tr.actual[1]*PriceBombay)+
  (tr.actual[2]*PriceCali)+
  (tr.actual[3]*PriceDermason)+
  (tr.actual[4]*PriceHoroz)+
  (tr.actual[5]*PriceSeker)+
  (tr.actual[6]*PriceSira)
tr.actual # Actual types of beans on train data
tr.actual.weightg #Actual weight in grams on train data
tr.actual.weightlb #predicted weight in lbs on train data
tr.actual.TotPr  #Total actual price 
```




## Total Price of Predicted Beans Class by LDA Method


```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by LDA method
lda.predicted=as.matrix(table(lda.pred.trn$class))

#predicted weight(in Grams) of all beans by LDA Method
ld.pred.weightg = (lda.predicted[1]*1.92)+
  (lda.predicted[2]*0.61)+
  (lda.predicted[3]*0.28)+
  (lda.predicted[4]*0.52)+
  (lda.predicted[5]*0.49)+
  (lda.predicted[6]*0.38)

#predicted weight(in lbs) of all beans by LDA Method
ld.pred.weightlb = ld.pred.weightg/453.592

#Total Predicted price by LDA Method
ld.pred.TotPr = (lda.predicted[1]*PriceBombay)+
  (lda.predicted[2]*PriceCali)+
  (lda.predicted[3]*PriceDermason)+
  (lda.predicted[4]*PriceHoroz)+
  (lda.predicted[5]*PriceSeker)+
  (lda.predicted[6]*PriceSira)
lda.predicted # total predicted types of beans by LDA
ld.pred.weightg #predicted weight in grams by LDA
ld.pred.weightlb #predicted weight in lbs by LDA
ld.pred.TotPr  #Total Predicted price by LDA Method
```

## Prediction on Test Data for LDA Method

```{r}
actual.tst.class <- test$Class
tst.df.lda <- as.data.frame(lda.pred.tst$class)
tst.df.lda$actual.tst.class <- actual.tst.class
```

## Total Price of Actual Beans Class on Test Data


```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#Actual quantity of all beans on train data
tst.actual=as.matrix(table(actual.tst.class))

#Actual weight(in Grams) of all beans on train Data
tst.actual.weightg = (tst.actual[1]*1.92)+
  (tst.actual[2]*0.61)+
  (tst.actual[3]*0.28)+
  (tst.actual[4]*0.52)+
  (tst.actual[5]*0.49)+
  (tst.actual[6]*0.38)

#Actual weight(in lbs) of all beans on train Data
tst.actual.weightlb = tst.actual.weightg/453.592

#Total price of Beans on Train Data
tst.actual.TotPr = (tst.actual[1]*PriceBombay)+
  (tst.actual[2]*PriceCali)+
  (tst.actual[3]*PriceDermason)+
  (tst.actual[4]*PriceHoroz)+
  (tst.actual[5]*PriceSeker)+
  (tst.actual[6]*PriceSira)
tst.actual # Actual types of beans on train data
tst.actual.weightg #Actual weight in grams on train data
tst.actual.weightlb #predicted weight in lbs on train data
tst.actual.TotPr  #Total actual price 
```

## Total Price of Predicted Beans Class by LDA Method on test data


```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by LDA method
tst.lda.predicted=as.matrix(table(lda.pred.tst$class))

#predicted weight(in Grams) of all beans by LDA Method
tst.ld.pred.weightg = (tst.lda.predicted[1]*1.92)+
  (tst.lda.predicted[2]*0.61)+
  (tst.lda.predicted[3]*0.28)+
  (tst.lda.predicted[4]*0.52)+
  (tst.lda.predicted[5]*0.49)+
  (tst.lda.predicted[6]*0.38)

#predicted weight(in lbs) of all beans by LDA Method
tst.ld.pred.weightlb = tst.ld.pred.weightg/453.592

#Total Predicted price by LDA Method
tst.ld.pred.TotPr = (tst.lda.predicted[1]*PriceBombay)+
  (tst.lda.predicted[2]*PriceCali)+
  (tst.lda.predicted[3]*PriceDermason)+
  (tst.lda.predicted[4]*PriceHoroz)+
  (tst.lda.predicted[5]*PriceSeker)+
  (tst.lda.predicted[6]*PriceSira)
tst.lda.predicted # total predicted types of beans by LDA
tst.ld.pred.weightg #predicted weight in grams by LDA
tst.ld.pred.weightlb #predicted weight in lbs by LDA
tst.ld.pred.TotPr  #Total Predicted price by LDA Method
```


##QDA Method

```{r}
##QDA Approach
quad.mod <- qda(Class~., train)
quad.mod
#Confusion matrix and accuracy – training data
pred.quad <- predict(quad.mod, train)$class
qda.pred.trn <- predict(quad.mod, train)
tab.quad <- table(Predicted = pred.quad, Actual = train$Class)
tab.quad
#write.csv(tab.quad,'qda_train.csv')
##Accuracy of the table
sum(diag(tab.quad))/sum(tab.quad)

#Confusion matrix and accuracy – testing data
pred.quad2 <- predict(quad.mod, test)$class
qda.pred.tst <- predict(quad.mod, test)
tab.quad2 <- table(Predicted = pred.quad2, Actual = test$Class)
tab.quad2
#write.csv(tab.quad2,'qda_test.csv')
sum(diag(tab.quad2))/sum(tab.quad2)
```



## Price Prediction Using QDA Model 


## Price prediction on Train Data

```{r}
actual.class <- train$Class
tr.df.qda <- as.data.frame(qda.pred.trn$class)
tr.df.qda$actual.class <- actual.class
head(tr.df.qda)
```


## Total Price of Predicted Beans Class by QDA Method on Train Data


```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by qda method
qda.predicted=as.matrix(table(qda.pred.trn$class))

#predicted weight(in Grams) of all beans by qda Method
qda.pred.weightg = (qda.predicted[1]*1.92)+
  (qda.predicted[2]*0.61)+
  (qda.predicted[3]*0.28)+
  (qda.predicted[4]*0.52)+
  (qda.predicted[5]*0.49)+
  (qda.predicted[6]*0.38)

#predicted weight(in lbs) of all beans by qda Method
qda.pred.weightlb = qda.pred.weightg/453.592

#Total Predicted price by qda Method
qda.pred.TotPr = (qda.predicted[1]*PriceBombay)+
  (qda.predicted[2]*PriceCali)+
  (qda.predicted[3]*PriceDermason)+
  (qda.predicted[4]*PriceHoroz)+
  (qda.predicted[5]*PriceSeker)+
  (qda.predicted[6]*PriceSira)
qda.predicted # total predicted types of beans by qda
qda.pred.weightg #predicted weight in grams by qda
qda.pred.weightlb #predicted weight in lbs by qda
qda.pred.TotPr  #Total Predicted price by qda Method
```

## Total Price of Predicted Beans Class by QDA Method on Test Data

```{r}
actual.tst.class <- test$Class
tst.df.qda <- as.data.frame(qda.pred.tst$class)
tst.df.qda$actual.tst.class <- actual.tst.class
```



```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by qdA method
tst.qda.predicted=as.matrix(table(qda.pred.tst$class))

#predicted weight(in Grams) of all beans by qdA Method
tst.qd.pred.weightg = (tst.qda.predicted[1]*1.92)+
  (tst.qda.predicted[2]*0.61)+
  (tst.qda.predicted[3]*0.28)+
  (tst.qda.predicted[4]*0.52)+
  (tst.qda.predicted[5]*0.49)+
  (tst.qda.predicted[6]*0.38)

#predicted weight(in lbs) of all beans by qdA Method
tst.qd.pred.weightlb = tst.qd.pred.weightg/453.592

#Total Predicted price by qdA Method
tst.qd.pred.TotPr = (tst.qda.predicted[1]*PriceBombay)+
  (tst.qda.predicted[2]*PriceCali)+
  (tst.qda.predicted[3]*PriceDermason)+
  (tst.qda.predicted[4]*PriceHoroz)+
  (tst.qda.predicted[5]*PriceSeker)+
  (tst.qda.predicted[6]*PriceSira)
tst.qda.predicted # total predicted types of beans by qdA
tst.qd.pred.weightg #predicted weight in grams by qdA
tst.qd.pred.weightlb #predicted weight in lbs by qdA
tst.qd.pred.TotPr  #Total Predicted price by qdA Method
```






## Mclust Method



```{r}
library(mclust)
cl_data <- train$Class
table(cl_data)
X <- train[,-8]
BIC <- mclustBIC(X)
plot(BIC)
```

```{r}
summary(BIC)
```

```{r}
mod1 <- Mclust(X, x = BIC)
summary(mod1, parameters = TRUE)
plot(mod1, what = "classification")
```


##MclustDA Method on Train Data

```{r}
#install.packages("mclust")
library(mclust)
#designed to estimate based rules, efficient way of fitting normals
X.dat=train[,-8] #subtract off the Class column
Class.dat=train[,8] # just the Class column

#g=1 specify max number of subgroups
mod.DA.G1= MclustDA(X.dat, Class.dat, G = 1,
                    modelType = "MclustDA")
summary(mod.DA.G1)



results.1=cbind(paste(predict.MclustDA(mod.DA.G1, 
                                       newdata = test[, -8])$classification), 
                paste(test[, 8]))

da.predicted.trn <- predict.MclustDA(mod.DA.G1, 
                                       newdata = train[, -8])$classification
mean(results.1[,1]==results.1[,2])
```



##MclustDA Method on Test Data

```{r}

#designed to estimate based rules, efficient way of fitting normals
test.mclustDA=test[,-8] #subtract off the Class column
Class.mclustDA=test[,8] # just the Class column

#g=1 specify max number of subgroups
mod.DA.tst= MclustDA(test.mclustDA, Class.mclustDA, G = 1,
                    modelType = "MclustDA")
summary(mod.DA.tst)

results.tst.DA=cbind(paste(predict.MclustDA(mod.DA.tst,
                                            newdata = test[, -8])$classification), 
                paste(test[, 8]))


da.predicted.tst <- predict.MclustDA(mod.DA.tst,
                                            newdata = test[, -8])$classification

mean(results.tst.DA[,1]==results.tst.DA[,2])

```




## Price Prediction Using MClustDA Model 

## Total Price of Predicted Beans Class by MClustDA Method on Train Data


```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by mclustDA method
mclustDA.predicted=as.matrix(table(da.predicted.trn))

#predicted weight(in Grams) of all beans by mclustDA Method
mclustDA.pred.weightg = (mclustDA.predicted[1]*1.92)+
  (mclustDA.predicted[2]*0.61)+
  (mclustDA.predicted[3]*0.28)+
  (mclustDA.predicted[4]*0.52)+
  (mclustDA.predicted[5]*0.49)+
  (mclustDA.predicted[6]*0.38)

#predicted weight(in lbs) of all beans by mclustDA Method
mclustDA.pred.weightlb = mclustDA.pred.weightg/453.592

#Total Predicted price by mclustDA Method
mclustDA.pred.TotPr = (mclustDA.predicted[1]*PriceBombay)+
  (mclustDA.predicted[2]*PriceCali)+
  (mclustDA.predicted[3]*PriceDermason)+
  (mclustDA.predicted[4]*PriceHoroz)+
  (mclustDA.predicted[5]*PriceSeker)+
  (mclustDA.predicted[6]*PriceSira)
mclustDA.predicted # total predicted types of beans by mclustDA
mclustDA.pred.weightg #predicted weight in grams by mclustDA
mclustDA.pred.weightlb #predicted weight in lbs by mclustDA
mclustDA.pred.TotPr  #Total Predicted price by mclustDA Method
```






## Total Price of Predicted Beans Class by MClustDA Method on Test Data

```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by mclustDA method
mclustDA.predicted.tst=as.matrix(table(da.predicted.tst))

#predicted weight(in Grams) of all beans by mclustDA Method
mclustDA.pred.weightg.tst = (mclustDA.predicted.tst[1]*1.92)+
  (mclustDA.predicted.tst[2]*0.61)+
  (mclustDA.predicted.tst[3]*0.28)+
  (mclustDA.predicted.tst[4]*0.52)+
  (mclustDA.predicted.tst[5]*0.49)+
  (mclustDA.predicted.tst[6]*0.38)

#predicted weight(in lbs) of all beans by mclustDA Method
mclustDA.pred.weightlb.tst = mclustDA.pred.weightg.tst/453.592

#Total Predicted price by mclustDA Method
mclustDA.pred.TotPr.tst = (mclustDA.predicted.tst[1]*PriceBombay)+
  (mclustDA.predicted.tst[2]*PriceCali)+
  (mclustDA.predicted.tst[3]*PriceDermason)+
  (mclustDA.predicted.tst[4]*PriceHoroz)+
  (mclustDA.predicted.tst[5]*PriceSeker)+
  (mclustDA.predicted.tst[6]*PriceSira)
mclustDA.predicted.tst # total predicted types of beans by mclustDA
mclustDA.pred.weightg.tst #predicted weight in grams by mclustDA
mclustDA.pred.weightlb.tst #predicted weight in lbs by mclustDA
mclustDA.pred.TotPr.tst  #Total Predicted price by mclustDA Method

```




## Mclust "EDDA" on Train Data


```{r}

#g=1 specify max number of subgroups
mod.EDDA.G1= MclustDA(X.dat, Class.dat, G = 1,
                      modelType = "EDDA")
summary(mod.EDDA.G1)
results.ED1=cbind(paste(predict.MclustDA(mod.EDDA.G1, 
                                       newdata = test[, -8])$classification), 
                paste(test[, 8]))

EDDA.predicted.trn <- predict.MclustDA(mod.EDDA.G1, 
                                       newdata = train[, -8])$classification

mean(results.ED1[,1]==results.ED1[,2])

```


```{r}
#designed to estimate based rules, efficient way of fitting normals
test.mclustEDDA=test[,-8] #subtract off the Class column
Class.mclustEDDA=test[,8] # just the Class column


#g=1 specify max number of subgroups
mod.EDDA.tst= MclustDA(test.mclustEDDA, Class.mclustEDDA, G = 1,
                      modelType = "EDDA")
summary(mod.EDDA.tst)
results.EDDA1=cbind(paste(predict.MclustDA(mod.EDDA.tst, 
                                       newdata = test[, -8])$classification), 
                paste(test[, 8]))

EDDA.predicted.tst <- predict.MclustDA(mod.EDDA.tst, 
                                       newdata = test[, -8])$classification

mean(results.EDDA1[,1]==results.EDDA1[,2])
```


## Price Prediction Using MClust 'EDDA' Model 

## Total Price of Predicted Beans Class by MClust 'EDDA' Method on Train Data


```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by mclustEDDA method
mclustEDDA.predicted=as.matrix(table(EDDA.predicted.trn))

#predicted weight(in Grams) of all beans by mclustEDDA Method
mclustEDDA.pred.weightg = (mclustEDDA.predicted[1]*1.92)+
  (mclustEDDA.predicted[2]*0.61)+
  (mclustEDDA.predicted[3]*0.28)+
  (mclustEDDA.predicted[4]*0.52)+
  (mclustEDDA.predicted[5]*0.49)+
  (mclustEDDA.predicted[6]*0.38)

#predicted weight(in lbs) of all beans by mclustEDDA Method
mclustEDDA.pred.weightlb = mclustEDDA.pred.weightg/453.592

#Total Predicted price by mclustEDDA Method
mclustEDDA.pred.TotPr = (mclustEDDA.predicted[1]*PriceBombay)+
  (mclustEDDA.predicted[2]*PriceCali)+
  (mclustEDDA.predicted[3]*PriceDermason)+
  (mclustEDDA.predicted[4]*PriceHoroz)+
  (mclustEDDA.predicted[5]*PriceSeker)+
  (mclustEDDA.predicted[6]*PriceSira)
mclustEDDA.predicted # total predicted types of beans by mclustEDDA
mclustEDDA.pred.weightg #predicted weight in grams by mclustEDDA
mclustEDDA.pred.weightlb #predicted weight in lbs by mclustEDDA
mclustEDDA.pred.TotPr  #Total Predicted price by mclustEDDA Method
```






## Total Price of Predicted Beans Class by MClustDA Method on Test Data

```{r}
#Bombay=1,Cali = 2 Dermanson = 3, Horoz=4, Seker =5, Sira = 6  

#predicted quantity of all beans by mclustEDDA method
mclustEDDA.predicted.tst=as.matrix(table(EDDA.predicted.tst))

#predicted weight(in Grams) of all beans by mclustEDDA Method
mclustEDDA.pred.weightg.tst = (mclustEDDA.predicted.tst[1]*1.92)+
  (mclustEDDA.predicted.tst[2]*0.61)+
  (mclustEDDA.predicted.tst[3]*0.28)+
  (mclustEDDA.predicted.tst[4]*0.52)+
  (mclustEDDA.predicted.tst[5]*0.49)+
  (mclustEDDA.predicted.tst[6]*0.38)

#predicted weight(in lbs) of all beans by mclustEDDA Method
mclustEDDA.pred.weightlb.tst = mclustEDDA.pred.weightg.tst/453.592

#Total Predicted price by mclustEDDA Method
mclustEDDA.pred.TotPr.tst = (mclustEDDA.predicted.tst[1]*PriceBombay)+
  (mclustEDDA.predicted.tst[2]*PriceCali)+
  (mclustEDDA.predicted.tst[3]*PriceDermason)+
  (mclustEDDA.predicted.tst[4]*PriceHoroz)+
  (mclustEDDA.predicted.tst[5]*PriceSeker)+
  (mclustEDDA.predicted.tst[6]*PriceSira)
mclustEDDA.predicted.tst # total predicted types of beans by mclustEDDA
mclustEDDA.pred.weightg.tst #predicted weight in grams by mclustEDDA
mclustEDDA.pred.weightlb.tst #predicted weight in lbs by mclustEDDA
mclustEDDA.pred.TotPr.tst  #Total Predicted price by mclustEDDA Method

```


## References

1. http://www.sthda.com/english/wiki/one-way-anova-test-in-r
2. https://www.guru99.com/r-anova-tutorial.html
3. https://gexijin.github.io/learnR/importing-data-and-managing-files.html
4. https://agroninfotech.blogspot.com/2021/11/rapid-publication-ready-anova-table-in-r.html
5. https://gexijin.github.io/learnR/index.html
6. https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/
7. https://rpubs.com/lmorgan95/ISLR_CH4_Solutions
8. https://rpubs.com/zlzlzl2/754880
9. https://rpubs.com/Richie222/853114
10. https://pages.cms.hu-berlin.de/EOL/gcg_quantitative-methods/Lab11_LDA_Model-assessment.html#Linear_Discriminant_Analysis
11. MCLust : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736/pdf/nihms793803.pdf
12. LDA: http://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html
13. https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/
14. https://sites.stat.washington.edu/mclust/
15. https://bradleyboehmke.github.io/HOML/model-clustering.html
16. Textbook: An Introduction to Statistical Learning With Appliaction in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani
17. STAT 602 and 601 Lecture Slides and Videos
18. https://alessiopassalacqua.github.io/getstrongeR-Rstats/readings.html
19. https://beta.rstudioconnect.com/content/2025/dplyr.nb.html
20. https://alessiopassalacqua.github.io/getstrongeR-Rstats/readings.html
21. https://www.webpages.uidaho.edu/~stevel/519/literatures/MCLUST%20for%20R.pdf
22. https://bradleyboehmke.github.io/HOML/model-clustering.html 




  


